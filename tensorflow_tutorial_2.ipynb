{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and visualize a model in Tensorflow - Part 2: Scikit Learn\n",
    "\n",
    "Along this tutorial we will explain the **multilayer perceptron** algorithm, which is the simplest possible form of an *artificial feed-forward neural network*. For this we will use the 20 newsgroup dataset obtained in the previous part of the tutorial.\n",
    "\n",
    "We will see the same algorithm in three different ways: using *scikit-learn*'s `MLPClassifier`, using *TensorFlow*'s `DNNClassifier`, and finally writing the whole neural network from scratch with the TensorFlow API.\n",
    "\n",
    "The idea is to compare how the different ways serve different purposes. This notebook deals with the simplest form possible using Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn\n",
    "\n",
    "Scikit Learn offers a simple API to do machine learning, specially in comparison to TensorFlow. The main problem with scikit learn is that most of the models are shallow ones (e.g. Logistic Regression, SVM, etc). The `MLPClassifier` exists and offers the possibility for a Neural Network classifier, however it is considerably slow to train a classifier and doesn't provide GPU optimization. The [documentation](http://scikit-learn.org/stable/modules/neural_networks_supervised.html) itself says the implementation of `MLPClassifier` is not intended for large-scale applications.\n",
    "\n",
    "To keep things simple we will create a simple multilayer perceptron with only one hidden layer with size 5000 (half the size of the input) and see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "newsgroups = np.load('./resources/newsgroup.npz')\n",
    "\n",
    "# Define the model\n",
    "model = MLPClassifier(\n",
    "    activation='relu',  # Rectifier Linear Unit activation\n",
    "    hidden_layer_sizes=(5000,),  # 1 hidden layer of size 5000\n",
    "    max_iter=5,  # Each epochs takes a lot of time so we keep it to 5\n",
    "    batch_size=100,  # The batch size is set to 100 elements\n",
    "    solver='adam')  # We use the adam solver\n",
    "\n",
    "model.fit(newsgroups['train_data'],\n",
    "          newsgroups['train_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96       160\n",
      "          1       0.82      0.84      0.83       195\n",
      "          2       0.89      0.80      0.84       197\n",
      "          3       0.73      0.82      0.77       196\n",
      "          4       0.90      0.83      0.87       192\n",
      "          5       0.87      0.93      0.90       196\n",
      "          6       0.84      0.87      0.85       194\n",
      "          7       0.92      0.91      0.92       198\n",
      "          8       0.97      0.96      0.97       199\n",
      "          9       0.99      0.98      0.99       199\n",
      "         10       0.99      0.97      0.98       200\n",
      "         11       0.99      0.93      0.96       198\n",
      "         12       0.90      0.89      0.90       196\n",
      "         13       0.93      0.96      0.95       198\n",
      "         14       0.98      0.96      0.97       197\n",
      "         15       0.98      0.94      0.96       200\n",
      "         16       0.93      0.95      0.94       182\n",
      "         17       0.99      0.98      0.98       188\n",
      "         18       0.90      0.92      0.91       155\n",
      "         19       0.85      0.90      0.88       126\n",
      "\n",
      "avg / total       0.92      0.92      0.92      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(\n",
    "    newsgroups['test_target'],\n",
    "    model.predict(newsgroups['test_data']))\n",
    "\n",
    "print(\"Accuracy: %.2f\" % accuracy)\n",
    "\n",
    "print(classification_report(\n",
    "    newsgroups['test_target'],\n",
    "    model.predict(newsgroups['test_data'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_edm]",
   "language": "python",
   "name": "conda-env-env_edm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
