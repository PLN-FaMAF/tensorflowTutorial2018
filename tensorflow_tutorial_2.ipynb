{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and visualize a model in Tensorflow - Part 2: The Basics of Tensorflow\n",
    "\n",
    "This notebook will explain the basics to create a linear model with Tensorflow. This part uses the 20 newsgroup dataset obtained in the previous part of the tutorial. Remember the dataset, comprised of documents, was converted to a matrix of document embeddings and splitted into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management\n",
    "\n",
    "Before creating the model, we need to specify what the input and output is going to be. For that we use the document matrix obtained in the previous part as input to the the classifier.\n",
    "\n",
    "However, most optimization algorithms similar to Stochastic Gradient Descent need the data in small portions for optimization purposes. On top of that, the training cycle goes through the entire dataset several times (epochs) before converging to a good solution.\n",
    "\n",
    "Fortunately, Tensorflow has the solution to iterate over datasets several times in small batches. These function are called input functions, and they can take a numpy array or a pandas dataframe. It's worth noticing that, during the past updates, Tensorflow has been including more functions to transform the input data in batches handling enconding of categorical features, embeddings, etc, althoug we wont use those function here.\n",
    "\n",
    "We load our dataset and create the input function to handle it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a numpy keyed structure\n",
    "newsgroups = np.load('./resources/newsgroup.npz')\n",
    "\n",
    "# Define the batch size and the number of labels\n",
    "batch_size = 100\n",
    "num_classes = newsgroups['labels'].shape[0]\n",
    "\n",
    "def dataset_input_fn(dataset):\n",
    "    \"\"\"\n",
    "    Creates an input function using the `numpy_input_fn` method from\n",
    "    tensorflow, based on the dataset we want to use.\n",
    "    \n",
    "    Args:\n",
    "        dataset: String that represents the dataset (should be `train` or `test`)\n",
    "    \n",
    "    Returns:\n",
    "        An `numpy_input_fn` function to feed to an estimator\n",
    "    \"\"\"\n",
    "    assert dataset in ('train', 'test'), \"The selected dataset should be `train` or `test`\"\n",
    "    \n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        # A dictionary of numpy arrays that match each array with the corresponding column in the model.\n",
    "        # For this case we only have \"one\" colum which represents all the dimensions in the embeddings.\n",
    "        x={'input_data': newsgroups['%s_data' % dataset]},\n",
    "        # The target array\n",
    "        y=newsgroups['%s_target' % dataset],\n",
    "        # The batch size to iterate the data in small fractions\n",
    "        batch_size=batch_size,\n",
    "        # If the dataset is `test` only run once\n",
    "        num_epochs=1 if dataset == 'test' else None,\n",
    "        # Only shuffle the dataset for the `train` data\n",
    "        shuffle=dataset == 'train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "The classifier to train is a `tf.estimator.LinearClassifier` which is basically a wrapper in Tensorflow for a Logistic Regression classifier. \n",
    "\n",
    "The object instantiation takes as input an iterator (i.e. `feature_columns`) that match the dictionary fed to the input function. As the input function only takes one column with a number of dimensions equal to the number of dimensions in the embeddings, there is only one feature column of that number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpuh7lem_4\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_save_checkpoints_secs': 600, '_service': None, '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_task_id': 0, '_task_type': 'worker', '_tf_random_seed': None, '_save_summary_steps': 100, '_model_dir': '/tmp/tmpuh7lem_4', '_num_worker_replicas': 1, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f76e1664fd0>, '_is_chief': True}\n"
     ]
    }
   ],
   "source": [
    "embedding_size = newsgroups['train_data'].shape[1]\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column('input_data', shape=(embedding_size,))]\n",
    "\n",
    "linear_classifier = tf.estimator.LinearClassifier(feature_columns=feature_columns, n_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cicle\n",
    "\n",
    "Now that we have the function that build the model, we can create the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpuh7lem_4/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 299.573\n",
      "INFO:tensorflow:global_step/sec: 153.097\n",
      "INFO:tensorflow:step = 101, loss = 110.593 (0.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.815\n",
      "INFO:tensorflow:step = 201, loss = 72.389 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.603\n",
      "INFO:tensorflow:step = 301, loss = 56.7456 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.033\n",
      "INFO:tensorflow:step = 401, loss = 58.9127 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.859\n",
      "INFO:tensorflow:step = 501, loss = 45.3894 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.64\n",
      "INFO:tensorflow:step = 601, loss = 30.6455 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.92\n",
      "INFO:tensorflow:step = 701, loss = 35.4754 (0.637 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.499\n",
      "INFO:tensorflow:step = 801, loss = 32.0218 (0.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.555\n",
      "INFO:tensorflow:step = 901, loss = 31.5903 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.775\n",
      "INFO:tensorflow:step = 1001, loss = 27.1715 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.432\n",
      "INFO:tensorflow:step = 1101, loss = 18.635 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.33\n",
      "INFO:tensorflow:step = 1201, loss = 23.2835 (0.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.968\n",
      "INFO:tensorflow:step = 1301, loss = 27.7314 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.69\n",
      "INFO:tensorflow:step = 1401, loss = 18.843 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.902\n",
      "INFO:tensorflow:step = 1501, loss = 14.8304 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.22\n",
      "INFO:tensorflow:step = 1601, loss = 23.8876 (0.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.072\n",
      "INFO:tensorflow:step = 1701, loss = 12.9912 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.639\n",
      "INFO:tensorflow:step = 1801, loss = 19.8055 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.123\n",
      "INFO:tensorflow:step = 1901, loss = 19.0631 (0.628 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmpuh7lem_4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 13.9887.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x7f76add3ec50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifier.train(input_fn=dataset_input_fn(\"train\"), steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-18-13:36:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpuh7lem_4/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-18-13:36:59\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.899894, average_loss = 0.427418, global_step = 2000, loss = 42.3593\n",
      "{'average_loss': 0.42741776, 'global_step': 2000, 'accuracy': 0.89989376, 'loss': 42.359348}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_results = linear_classifier.evaluate(input_fn=dataset_input_fn(\"test\"))\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
