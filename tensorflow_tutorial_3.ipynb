{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and visualize a model in Tensorflow - Part 3: TensorFlow DNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management\n",
    "\n",
    "Before creating the model, we need to specify what the input and output is going to be. For that we use the document matrix obtained in the previous part as input to the the classifier.\n",
    "\n",
    "However, most optimization algorithms similar to Stochastic Gradient Descent need the data in small portions for optimization purposes. On top of that, the training cycle goes through the entire dataset several times (epochs) before converging to a good solution.\n",
    "\n",
    "Fortunately, Tensorflow has the solution to iterate over datasets several times in small batches. These function are called input functions, and they can take a numpy array or a pandas dataframe. It's worth noticing that, during the past updates, Tensorflow has been including more functions to transform the input data in batches handling enconding of categorical features, embeddings, etc, althoug we wont use those function here.\n",
    "\n",
    "We load our dataset and create the input function to handle it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a numpy keyed structure\n",
    "newsgroups = np.load('./resources/newsgroup.npz')\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 100\n",
    "\n",
    "def dataset_input_fn(dataset):\n",
    "    \"\"\"\n",
    "    Creates an input function using the `numpy_input_fn` method from\n",
    "    tensorflow, based on the dataset we want to use.\n",
    "    \n",
    "    Args:\n",
    "        dataset: String that represents the dataset (should be `train` or `test`)\n",
    "    \n",
    "    Returns:\n",
    "        An `numpy_input_fn` function to feed to an estimator\n",
    "    \"\"\"\n",
    "    assert dataset in ('train', 'test'), \"The selected dataset should be `train` or `test`\"\n",
    "    \n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        # A dictionary of numpy arrays that match each array with the corresponding column in the model.\n",
    "        # For this case we only have \"one\" colum which represents all the dimensions in the embeddings.\n",
    "        x={'input_data': newsgroups['%s_data' % dataset]},\n",
    "        # The target array\n",
    "        y=newsgroups['%s_target' % dataset],\n",
    "        # The batch size to iterate the data in small fractions\n",
    "        batch_size=batch_size,\n",
    "        # If the dataset is `test` only run once\n",
    "        num_epochs=1 if dataset == 'test' else None,\n",
    "        # Only shuffle the dataset for the `train` data\n",
    "        shuffle=dataset == 'train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "The classifier to train is a `tf.estimator.LinearClassifier` which is basically a wrapper in Tensorflow for a Logistic Regression classifier. \n",
    "\n",
    "The object instantiation takes as input an iterator (i.e. `feature_columns`) that match the dictionary fed to the input function. As the input function only takes one column with a number of dimensions equal to the number of dimensions in the embeddings, there is only one feature column of that number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x14828a693fd0>, '_num_worker_replicas': 1, '_master': '', '_num_ps_replicas': 0, '_model_dir': '/tmp/ng_model', '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_service': None, '_is_chief': True, '_session_config': None, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_task_id': 0, '_tf_random_seed': None}\n"
     ]
    }
   ],
   "source": [
    "input_size = newsgroups['train_data'].shape[1]\n",
    "num_classes = newsgroups['labels'].shape[0]\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column('input_data', shape=(input_size,))]\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=(5000,),\n",
    "    n_classes=num_classes,\n",
    "    model_dir=\"/tmp/ng_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cicle\n",
    "\n",
    "Now that we have the function that build the model, we can create the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/ng_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 299.54712, step = 1\n",
      "INFO:tensorflow:global_step/sec: 37.6368\n",
      "INFO:tensorflow:loss = 46.64124, step = 101 (2.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.5234\n",
      "INFO:tensorflow:loss = 16.212477, step = 201 (2.596 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.8718\n",
      "INFO:tensorflow:loss = 10.498708, step = 301 (2.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.5528\n",
      "INFO:tensorflow:loss = 1.0421454, step = 401 (2.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.6393\n",
      "INFO:tensorflow:loss = 0.9621756, step = 501 (2.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.4342\n",
      "INFO:tensorflow:loss = 0.6435354, step = 601 (2.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.7656\n",
      "INFO:tensorflow:loss = 0.5751495, step = 701 (2.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9498\n",
      "INFO:tensorflow:loss = 0.4871225, step = 801 (2.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7682\n",
      "INFO:tensorflow:loss = 0.49556798, step = 901 (2.796 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.5221\n",
      "INFO:tensorflow:loss = 0.28662223, step = 1001 (2.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.8231\n",
      "INFO:tensorflow:loss = 0.23561649, step = 1101 (2.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.9526\n",
      "INFO:tensorflow:loss = 0.2804272, step = 1201 (2.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.3997\n",
      "INFO:tensorflow:loss = 0.24313796, step = 1301 (2.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.6094\n",
      "INFO:tensorflow:loss = 0.21590081, step = 1401 (2.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.164\n",
      "INFO:tensorflow:loss = 6.154641, step = 1501 (2.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7318\n",
      "INFO:tensorflow:loss = 0.21628344, step = 1601 (2.799 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.6705\n",
      "INFO:tensorflow:loss = 0.15349889, step = 1701 (2.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.8667\n",
      "INFO:tensorflow:loss = 0.20225531, step = 1801 (2.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9579\n",
      "INFO:tensorflow:loss = 0.20444003, step = 1901 (2.782 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/ng_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.21131364.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x14828a66c160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=dataset_input_fn(\"train\"), steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-19-13:50:37\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ng_model/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-13:50:38\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9065321, average_loss = 0.33596954, global_step = 2000, loss = 33.29635\n",
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_results = model.evaluate(input_fn=dataset_input_fn(\"test\"))\n",
    "print(\"Accuracy: %.2f\" % eval_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/ng_model/model.ckpt-2000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.96      0.94       160\n",
      "          1       0.80      0.87      0.83       195\n",
      "          2       0.86      0.84      0.85       197\n",
      "          3       0.83      0.78      0.80       196\n",
      "          4       0.91      0.83      0.87       192\n",
      "          5       0.88      0.91      0.89       196\n",
      "          6       0.88      0.78      0.83       194\n",
      "          7       0.89      0.93      0.91       198\n",
      "          8       0.99      0.95      0.97       199\n",
      "          9       0.95      0.97      0.96       199\n",
      "         10       0.98      0.95      0.97       200\n",
      "         11       0.98      0.94      0.96       198\n",
      "         12       0.76      0.90      0.83       196\n",
      "         13       0.92      0.96      0.94       198\n",
      "         14       0.98      0.95      0.97       197\n",
      "         15       0.94      0.94      0.94       200\n",
      "         16       0.95      0.92      0.94       182\n",
      "         17       0.97      0.97      0.97       188\n",
      "         18       0.88      0.93      0.90       155\n",
      "         19       0.89      0.82      0.85       126\n",
      "\n",
      "avg / total       0.91      0.91      0.91      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = list(model.predict(input_fn=dataset_input_fn(\"test\")))\n",
    "test_predictions_classes = np.array([p['class_ids'][0] for p in test_predictions])\n",
    "\n",
    "print(classification_report(newsgroups['test_target'], test_predictions_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-tutorial]",
   "language": "python",
   "name": "conda-env-tensorflow-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
