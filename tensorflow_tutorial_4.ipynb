{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and visualize a model in Tensorflow - Part 4: TensorFlow DNNClassifier\n",
    "\n",
    "This is the last part of the tutorial regarding the training of a model using TensorFlow. In [Part 3](https://github.com/PLN-FaMAF/tensorflowTutorial2018/blob/master/tensorflow_tutorial_3.ipynb) we saw how to design, train and evaluate a neural network using the TensorFlow's API. That method is highly customizable and flexible, but is also tedious to work with.\n",
    "\n",
    "When TensorFlow first came out, the previous method was the only way to create a model. Luckily, newer versions of the library are shipped with some packages that are simpler to work with. In particular the [estimator's API](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) which provides a much simpler way to define a model without having to work with the math behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management\n",
    "\n",
    "Before creating the model, we need to specify what the input and output is going to be. For that we use the document matrix obtained in the previous part as input to the the classifier.\n",
    "\n",
    "However, most optimization algorithms similar to Stochastic Gradient Descent need the data in small portions for optimization purposes. On top of that, the training cycle goes through the entire dataset several times (epochs) before converging to a good solution.\n",
    "\n",
    "Fortunately, Tensorflow has the solution to iterate over datasets several times in small batches. These function are called input functions, and they can take a numpy array or a pandas dataframe. It's worth noticing that, during the past updates, Tensorflow has been including more functions to transform the input data in batches handling enconding of categorical features, embeddings, etc, althoug we wont use those function here.\n",
    "\n",
    "We load our dataset and create the input function to handle it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a numpy keyed structure\n",
    "newsgroups = np.load('./resources/newsgroup.npz')\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 100\n",
    "\n",
    "def dataset_input_fn(dataset):\n",
    "    \"\"\"\n",
    "    Creates an input function using the `numpy_input_fn` method from\n",
    "    tensorflow, based on the dataset we want to use.\n",
    "    \n",
    "    Args:\n",
    "        dataset: String that represents the dataset\n",
    "        (should be `train` or `test`)\n",
    "    \n",
    "    Returns:\n",
    "        An `numpy_input_fn` function to feed to an estimator\n",
    "    \"\"\"\n",
    "    assert dataset in ('train', 'test'),\\\n",
    "        \"The selected dataset should be `train` or `test`\"\n",
    "    \n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        # A dictionary of numpy arrays that match each array with the\n",
    "        # corresponding column in the model. For this case we only\n",
    "        # have \"one\" colum which represents the whole array.\n",
    "        x={'input_data': newsgroups['%s_data' % dataset]},\n",
    "        # The target array\n",
    "        y=newsgroups['%s_target' % dataset],\n",
    "        # The batch size to iterate the data in small fractions\n",
    "        batch_size=batch_size,\n",
    "        # If the dataset is `test` only run once\n",
    "        num_epochs=1 if dataset == 'test' else None,\n",
    "        # Only shuffle the dataset for the `train` data\n",
    "        shuffle=dataset == 'train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "The classifier to train is a `tf.estimator.LinearClassifier` which is basically a wrapper in Tensorflow for a Logistic Regression classifier. \n",
    "\n",
    "The object instantiation takes as input an iterator (i.e. `feature_columns`) that match the dictionary fed to the input function. As the input function only takes one column with a number of dimensions equal to the number of dimensions in the embeddings, there is only one feature column of that number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_num_ps_replicas': 0, '_tf_random_seed': None, '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_task_type': 'worker', '_model_dir': '/tmp/ng_model', '_save_checkpoints_steps': None, '_task_id': 0, '_service': None, '_keep_checkpoint_max': 5, '_master': '', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x146cc7350eb8>, '_log_step_count_steps': 100, '_save_checkpoints_secs': 600, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "input_size = newsgroups['train_data'].shape[1]\n",
    "num_classes = newsgroups['labels'].shape[0]\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(\n",
    "    'input_data', shape=(input_size,))]\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=(5000, 2000,),\n",
    "    n_classes=num_classes,\n",
    "    model_dir=\"/tmp/ng_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cicle\n",
    "\n",
    "Now that we have the function that build the model, we can create the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ng_model/model.ckpt-2\n",
      "INFO:tensorflow:Saving checkpoints for 3 into /tmp/ng_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 375.80908, step = 3\n",
      "INFO:tensorflow:global_step/sec: 30.59\n",
      "INFO:tensorflow:loss = 82.62279, step = 103 (3.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.1716\n",
      "INFO:tensorflow:loss = 23.374432, step = 203 (3.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.548\n",
      "INFO:tensorflow:loss = 5.9647384, step = 303 (3.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.3147\n",
      "INFO:tensorflow:loss = 1.7345433, step = 403 (3.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.7272\n",
      "INFO:tensorflow:loss = 0.7357566, step = 503 (3.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.6862\n",
      "INFO:tensorflow:loss = 0.6507288, step = 603 (3.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.8988\n",
      "INFO:tensorflow:loss = 0.24774258, step = 703 (3.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.1994\n",
      "INFO:tensorflow:loss = 0.25066963, step = 803 (3.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.5604\n",
      "INFO:tensorflow:loss = 2.9760516, step = 903 (3.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.5273\n",
      "INFO:tensorflow:loss = 0.09226587, step = 1003 (3.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.0575\n",
      "INFO:tensorflow:loss = 0.09409205, step = 1103 (3.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.4872\n",
      "INFO:tensorflow:loss = 0.0986183, step = 1203 (3.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.2357\n",
      "INFO:tensorflow:loss = 0.04505531, step = 1303 (3.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8515\n",
      "INFO:tensorflow:loss = 0.068174876, step = 1403 (3.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.237\n",
      "INFO:tensorflow:loss = 0.043803487, step = 1503 (3.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.8998\n",
      "INFO:tensorflow:loss = 0.034229655, step = 1603 (3.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.9349\n",
      "INFO:tensorflow:loss = 0.05760591, step = 1703 (3.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.9506\n",
      "INFO:tensorflow:loss = 0.063201495, step = 1803 (3.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.6258\n",
      "INFO:tensorflow:loss = 0.03050931, step = 1903 (3.265 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2002 into /tmp/ng_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.050210044.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x146cc7350e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=dataset_input_fn(\"train\"), steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-05-21:15:10\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ng_model/model.ckpt-2002\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-05-21:15:11\n",
      "INFO:tensorflow:Saving dict for global step 2002: accuracy = 0.8951142, average_loss = 0.45907655, global_step = 2002, loss = 45.496902\n",
      "Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_results = model.evaluate(\n",
    "    input_fn=dataset_input_fn(\"test\"))\n",
    "print(\"Accuracy: %.2f\" % eval_results['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use the same tools from scikit-learn that we use for any other model, once we have the array with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/ng_model/model.ckpt-2002\n",
      "Classification Report\n",
      "=====================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.95      0.93       160\n",
      "          1       0.83      0.85      0.84       195\n",
      "          2       0.81      0.85      0.83       197\n",
      "          3       0.78      0.73      0.75       196\n",
      "          4       0.87      0.80      0.83       192\n",
      "          5       0.90      0.89      0.89       196\n",
      "          6       0.84      0.80      0.82       194\n",
      "          7       0.85      0.91      0.88       198\n",
      "          8       0.97      0.92      0.95       199\n",
      "          9       0.96      0.97      0.96       199\n",
      "         10       0.98      0.96      0.97       200\n",
      "         11       0.99      0.93      0.96       198\n",
      "         12       0.78      0.90      0.83       196\n",
      "         13       0.87      0.95      0.91       198\n",
      "         14       0.95      0.95      0.95       197\n",
      "         15       0.98      0.92      0.95       200\n",
      "         16       0.94      0.90      0.92       182\n",
      "         17       0.97      0.96      0.97       188\n",
      "         18       0.88      0.92      0.90       155\n",
      "         19       0.85      0.83      0.84       126\n",
      "\n",
      "avg / total       0.90      0.90      0.90      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions =\\\n",
    "    list(model.predict(input_fn=dataset_input_fn(\"test\")))\n",
    "test_predictions_classes = np.array(\n",
    "    [p['class_ids'][0] for p in test_predictions])\n",
    "\n",
    "print(\"Classification Report\\n=====================\")\n",
    "print(classification_report(\n",
    "    newsgroups['test_target'], test_predictions_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-tutorial]",
   "language": "python",
   "name": "conda-env-tensorflow-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
