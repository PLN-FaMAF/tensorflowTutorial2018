{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and visualize a model in Tensorflow - Part 3: TensorFlow DNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management\n",
    "\n",
    "Before creating the model, we need to specify what the input and output is going to be. For that we use the document matrix obtained in the previous part as input to the the classifier.\n",
    "\n",
    "However, most optimization algorithms similar to Stochastic Gradient Descent need the data in small portions for optimization purposes. On top of that, the training cycle goes through the entire dataset several times (epochs) before converging to a good solution.\n",
    "\n",
    "Fortunately, Tensorflow has the solution to iterate over datasets several times in small batches. These function are called input functions, and they can take a numpy array or a pandas dataframe. It's worth noticing that, during the past updates, Tensorflow has been including more functions to transform the input data in batches handling enconding of categorical features, embeddings, etc, althoug we wont use those function here.\n",
    "\n",
    "We load our dataset and create the input function to handle it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a numpy keyed structure\n",
    "newsgroups = np.load('./resources/newsgroup.npz')\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 100\n",
    "\n",
    "def dataset_input_fn(dataset):\n",
    "    \"\"\"\n",
    "    Creates an input function using the `numpy_input_fn` method from\n",
    "    tensorflow, based on the dataset we want to use.\n",
    "    \n",
    "    Args:\n",
    "        dataset: String that represents the dataset (should be `train` or `test`)\n",
    "    \n",
    "    Returns:\n",
    "        An `numpy_input_fn` function to feed to an estimator\n",
    "    \"\"\"\n",
    "    assert dataset in ('train', 'test'), \"The selected dataset should be `train` or `test`\"\n",
    "    \n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        # A dictionary of numpy arrays that match each array with the\n",
    "        # corresponding column in the model. For this case we only\n",
    "        # have \"one\" colum which represents the whole array.\n",
    "        x={'input_data': newsgroups['%s_data' % dataset]},\n",
    "        # The target array\n",
    "        y=newsgroups['%s_target' % dataset],\n",
    "        # The batch size to iterate the data in small fractions\n",
    "        batch_size=batch_size,\n",
    "        # If the dataset is `test` only run once\n",
    "        num_epochs=1 if dataset == 'test' else None,\n",
    "        # Only shuffle the dataset for the `train` data\n",
    "        shuffle=dataset == 'train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "The classifier to train is a `tf.estimator.LinearClassifier` which is basically a wrapper in Tensorflow for a Logistic Regression classifier. \n",
    "\n",
    "The object instantiation takes as input an iterator (i.e. `feature_columns`) that match the dictionary fed to the input function. As the input function only takes one column with a number of dimensions equal to the number of dimensions in the embeddings, there is only one feature column of that number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_num_worker_replicas': 1, '_save_checkpoints_steps': None, '_task_id': 0, '_service': None, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/ng_model', '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1480a7829ef0>, '_task_type': 'worker', '_tf_random_seed': None, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "input_size = newsgroups['train_data'].shape[1]\n",
    "num_classes = newsgroups['labels'].shape[0]\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(\n",
    "    'input_data', shape=(input_size,))]\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=(5000, 2000,),\n",
    "    n_classes=num_classes,\n",
    "    model_dir=\"/tmp/ng_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cicle\n",
    "\n",
    "Now that we have the function that build the model, we can create the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/ng_model/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 299.67517\n",
      "INFO:tensorflow:global_step/sec: 36.4463\n",
      "INFO:tensorflow:step = 101, loss = 84.466484 (2.744 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.1879\n",
      "INFO:tensorflow:step = 201, loss = 43.79915 (2.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2797\n",
      "INFO:tensorflow:step = 301, loss = 9.824969 (2.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3661\n",
      "INFO:tensorflow:step = 401, loss = 1.3690943 (2.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.0466\n",
      "INFO:tensorflow:step = 501, loss = 0.45615438 (2.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.0685\n",
      "INFO:tensorflow:step = 601, loss = 0.24632725 (2.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.9918\n",
      "INFO:tensorflow:step = 701, loss = 0.22747818 (2.859 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8299\n",
      "INFO:tensorflow:step = 801, loss = 0.11410611 (2.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.9194\n",
      "INFO:tensorflow:step = 901, loss = 0.15321907 (2.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.86\n",
      "INFO:tensorflow:step = 1001, loss = 0.0934096 (2.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3274\n",
      "INFO:tensorflow:step = 1101, loss = 0.06898642 (2.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7625\n",
      "INFO:tensorflow:step = 1201, loss = 0.09228238 (2.796 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.0152\n",
      "INFO:tensorflow:step = 1301, loss = 2.0747812 (2.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.3977\n",
      "INFO:tensorflow:step = 1401, loss = 0.04850391 (2.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.5899\n",
      "INFO:tensorflow:step = 1501, loss = 0.063759446 (2.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5649\n",
      "INFO:tensorflow:step = 1601, loss = 0.028750144 (2.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.3189\n",
      "INFO:tensorflow:step = 1701, loss = 0.024792122 (3.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.387\n",
      "INFO:tensorflow:step = 1801, loss = 0.035809 (2.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.9954\n",
      "INFO:tensorflow:step = 1901, loss = 0.035494316 (3.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.4856\n",
      "INFO:tensorflow:step = 2001, loss = 0.029636107 (2.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.0197\n",
      "INFO:tensorflow:step = 2101, loss = 1.562786 (3.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.3402\n",
      "INFO:tensorflow:step = 2201, loss = 0.027354028 (3.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.1471\n",
      "INFO:tensorflow:step = 2301, loss = 0.027135441 (3.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.6493\n",
      "INFO:tensorflow:step = 2401, loss = 0.03156532 (3.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.471\n",
      "INFO:tensorflow:step = 2501, loss = 0.02601598 (2.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.1576\n",
      "INFO:tensorflow:step = 2601, loss = 0.016511329 (2.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.0915\n",
      "INFO:tensorflow:step = 2701, loss = 0.019270804 (3.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.2812\n",
      "INFO:tensorflow:step = 2801, loss = 0.017134534 (3.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.159\n",
      "INFO:tensorflow:step = 2901, loss = 0.020643404 (3.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.0419\n",
      "INFO:tensorflow:step = 3001, loss = 0.038790133 (2.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8696\n",
      "INFO:tensorflow:step = 3101, loss = 0.015596995 (3.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.4371\n",
      "INFO:tensorflow:step = 3201, loss = 0.011141341 (3.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.3989\n",
      "INFO:tensorflow:step = 3301, loss = 0.019784672 (3.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8012\n",
      "INFO:tensorflow:step = 3401, loss = 0.015686464 (3.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.3591\n",
      "INFO:tensorflow:step = 3501, loss = 0.012549947 (3.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.6579\n",
      "INFO:tensorflow:step = 3601, loss = 0.013693469 (3.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.0623\n",
      "INFO:tensorflow:step = 3701, loss = 0.015417066 (3.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.417\n",
      "INFO:tensorflow:step = 3801, loss = 0.008883728 (3.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.5969\n",
      "INFO:tensorflow:step = 3901, loss = 0.019171689 (3.268 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /tmp/ng_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.013829386.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1480a78050f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=dataset_input_fn(\"train\"), steps=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-19-14:23:21\n",
      "INFO:tensorflow:Restoring parameters from /tmp/ng_model/model.ckpt-4000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-14:23:22\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.9020181, average_loss = 0.47777027, global_step = 4000, loss = 47.34955\n",
      "Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_results = model.evaluate(input_fn=dataset_input_fn(\"test\"))\n",
    "print(\"Accuracy: %.2f\" % eval_results['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use the same tools from scikit-learn that we use for any other model, once we have the array with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/ng_model/model.ckpt-4000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.96      0.94       160\n",
      "          1       0.88      0.81      0.84       195\n",
      "          2       0.82      0.87      0.84       197\n",
      "          3       0.77      0.80      0.78       196\n",
      "          4       0.92      0.80      0.86       192\n",
      "          5       0.89      0.92      0.90       196\n",
      "          6       0.89      0.82      0.86       194\n",
      "          7       0.91      0.91      0.91       198\n",
      "          8       0.97      0.96      0.97       199\n",
      "          9       0.96      0.96      0.96       199\n",
      "         10       0.98      0.94      0.96       200\n",
      "         11       0.97      0.94      0.96       198\n",
      "         12       0.77      0.89      0.82       196\n",
      "         13       0.89      0.97      0.93       198\n",
      "         14       0.95      0.94      0.95       197\n",
      "         15       0.94      0.94      0.94       200\n",
      "         16       0.92      0.90      0.91       182\n",
      "         17       0.97      0.96      0.97       188\n",
      "         18       0.91      0.92      0.91       155\n",
      "         19       0.83      0.78      0.80       126\n",
      "\n",
      "avg / total       0.90      0.90      0.90      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = list(model.predict(input_fn=dataset_input_fn(\"test\")))\n",
    "test_predictions_classes = np.array(\n",
    "    [p['class_ids'][0] for p in test_predictions])\n",
    "\n",
    "print(classification_report(\n",
    "    newsgroups['test_target'], test_predictions_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_edm]",
   "language": "python",
   "name": "conda-env-env_edm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
